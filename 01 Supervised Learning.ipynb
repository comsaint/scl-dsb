{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to the first notebook!\n",
    "\n",
    "In this session we will work on a real-world dataset related to prediction of house price in a district.\n",
    "\n",
    "The first thing you will do is to know how to run the code... see the `Run` button above?\n",
    "\n",
    "You can also use the shortcut Ctrl + Enter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the first block of Python code to run - do not worry about what it means, these are mostly utilities and importing useful packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "rnd = 1\n",
    "np.random.seed(rnd)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"1\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset of California census data 1990."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_PATH = os.path.join(\"data\", \"housing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the data is loaded, which is named `housing`. `housing` is a Pandas DataFrame object which resembles your (familiar) Excel sheet.  \n",
    "\n",
    "Let's look at the first 5 rows of data with the `head` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pandas` provides a lot of methods to interact and summarize data. For example, we can check on the basic statistics of each feature using the `describe` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing speaks more than a beautiful picture. `Pandas` provides some plotting methods, but there are a lot of Python packages specialised on drawing graphs, for example `matplotlib`, `seaborn`, `plotly` and `bokeh`, just to name a few.\n",
    "\n",
    "Let's start with something simple: since we have the Latitude and Longitude, what about a plot on the map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 3D\n",
    "# each dot is one sample on map, size of a dot proportional to population and color represents media house price.\n",
    "# you may try to overlay an actual geographic map below the points!\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n",
    "s=housing[\"population\"]/100, label=\"population\", figsize=(15,10),\n",
    "c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    ") \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning - you should NOT do this!\n",
    "In a real machine learning project, we should plot a lot and a lot of graphs to look for abnormality, understand the physical meaning, gain insight, and develop questions etc. But for the sake of the moment, we will jump into modeling and come back later.\n",
    "### End of Warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will only use 1,000 rows of the data (i.e. 1,000 samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's show the top 5 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a subset of 1,000 samples\n",
    "housing = housing.sample(n=1000, random_state=np.random.seed(rnd))\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quick way to look for missing value: the `info` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the dataset contains 1K rows (samples) with 10 attributes. \n",
    "\n",
    "Note there is one categorical variable `ocean_proximity`. (scroll to the right if you cannot see it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this exercise, we will build a model to predict the `median_house_value`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by plotting a histogram of our 'target': `median_house_value`. There is a `hist` method that comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['median_house_value'].hist(bins=50)  # 50 bins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the suspicious spike near $500K? In practice we should go back to the data soucrce and look for the reason. It may even redefine our goal and problem!\n",
    "\n",
    "But for now, let's simply filter them out to simplifiy the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out samples with median_house_value >= 500,000\n",
    "housing_flt = housing[housing['median_house_value'] < 500000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_flt['median_house_value'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet another handy method (actually a `property`, to be precise) to get the shape of data: `shape`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have filtered out 35 (1000-965) records\n",
    "housing_flt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_flt.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...how about listing the linear correlation between our target and each feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reflection: does the correlation make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the data, what's next?\n",
    "\n",
    "Before we dive in, there is a very important step to do: let's split the data set into 2, one for training the model (commonly called the * *training set* *) and the other for evaluation (the * *test set* *).\n",
    "\n",
    "This is **VERY IMPORTANT** in modeling - we need to hold out a set of data, which is not known by the model, to evaluate how well the model perform.\n",
    "\n",
    "An analogue, in school - we are *__trained__* with materials in lessons (reading, lectures...). To evaluate how well we understand the materials, we are *__tested__* in with materials that are relevant, but not known in advance (i.e. examination). Otherwise we will simply recite (* *memorize* *) the questions and answers in exam and get a good score, without actually understand the materials at all!\n",
    "\n",
    "### Question: what problem will it pose if a model has access to the test data during training?\n",
    "Hint: you will know in coming lecture!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set (90:10 split)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(housing_flt, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, time to begin modeling!\n",
    "\n",
    "As a start, let's use the only 1 feature, `median_income`, to predict our target (label), `median_house_value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only 2 columns, \"median_income\" and \"median_house_value\"\n",
    "usecols = ['median_income', 'median_house_value']\n",
    "train_set_univariate = train_set[usecols]\n",
    "test_set_univariate = test_set[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a plot first...\n",
    "train_set_univariate.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, the relationship looks quite linear... (expected, it is what our correlation table above told us). \n",
    "\n",
    "Therefore, how about we start with one of the most common and simplest model, the *Univariate Linear Regression*?\n",
    "\n",
    "### Question: what is the formula of a simple (univariate) linear model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a Python package, Scikit-Learn `sklearn`, for the model training etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# import an error metric (in this case the mean squared error, MSE)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_set_univariate__X = np.array(train_set_univariate['median_income']).reshape(-1, 1)  # preparing feature (data X)\n",
    "train_set_univariate__y = train_set_univariate['median_house_value']  # preparing the target (y)\n",
    "\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X=train_set_univariate__X, y=train_set_univariate__y)  # feed X and y into the linear model, ask it to 'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained. (Yes, done!)\n",
    "\n",
    "Let's show its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope `m`: {:,.2f}\\nIntercept `c`: {:,.2f}\".format(linear_reg.coef_[0], linear_reg.intercept_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: what is the formula of the trained model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test how well our model is doing - as the first thing, how about applying this model on the training data, get the predictions, and draw some graphs?\n",
    "\n",
    "### Warning: this is NOT the correct way to evaluate a model - we should use a validation or test set. This is only for sake of illusration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for train set\n",
    "train_set_univariate__y_pred = linear_reg.predict(train_set_univariate__X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a grahp\n",
    "train_set_univariate.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.4)\n",
    "plt.plot(train_set_univariate__X, train_set_univariate__y_pred, color='red', linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "from sklearn import metrics\n",
    "\n",
    "def print_eval(y_true, y_pred):\n",
    "    print('Average target in train set: {:,.0f}'.format(y_true.mean()))\n",
    "    print('Mean Absolute Error on train set: {:,.0f}'.format(metrics.mean_absolute_error(y_true, y_pred)))\n",
    "    print('Root Mean Squared Error on train set: {:,.0f}'.format(np.sqrt(metrics.mean_squared_error(y_true, y_pred))))\n",
    "    print('Relative Root Mean Squared Error on train set: {:.2f} %'.format(100 * np.sqrt(metrics.mean_squared_error(y_true, y_pred))/y_true.mean()))\n",
    "    print('R^2 of train set: {:,.2f}'.format(metrics.r2_score(y_true, y_pred)))\n",
    "    return None\n",
    "\n",
    "print_eval(train_set_univariate__y, train_set_univariate__y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "In the example above, the predictions (red line) seems skewed towards clockwise direction, probably due to the few data points with high `median_income`.\n",
    "\n",
    "Let's assume the data points with `median_income` > 9 are outlier, repeat the modeling process (prepare data, train model) above and draw a graph of the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Improve model performance\n",
    "\n",
    "The performace is not great - an RMSE of ~70K to a target of ~200K is a bit too high, even on train set...\n",
    "\n",
    "What can we do to improve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things we can try:\n",
    "\n",
    "1. Collect more samples\n",
    "2. Try a different combination of features\n",
    "3. Try a different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try #2 and use more features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try adding a new feature, 'households'\n",
    "usecols = ['median_income', 'housing_median_age', 'median_house_value']\n",
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(housing_flt[usecols], alpha=0.2, figsize=(15, 12), diagonal='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, prepare train and test sets\n",
    "train_set_bivariates = train_set[usecols]\n",
    "test_set_bivariates = test_set[usecols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train_set_bivariates__X = train_set_bivariates[['median_income', 'housing_median_age']]\n",
    "train_set_bivariates__y = train_set_bivariates['median_house_value']\n",
    "\n",
    "linear_reg_2 = LinearRegression()\n",
    "linear_reg_2.fit(X=train_set_bivariates__X, y=train_set_bivariates__y)\n",
    "\n",
    "# get predictions\n",
    "train_set_bivariates__pred = linear_reg_2.predict(train_set_bivariates__X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slopes m1 and m2: {:,.2f}, {:,.2f}\\nIntercept: {:,.2f}\".format(linear_reg_2.coef_[0], linear_reg_2.coef_[1], linear_reg_2.intercept_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_eval(train_set_bivariates__y, train_set_bivariates__pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
